from db.db_services.llm_service import LLMBundle, TenantLLMService
from db.constants import LLMType, ParserType
from openai import OpenAI
import openai
from textwrap import dedent
from db.db_services.knowledgebase_service import KnowledgebaseService
from db.db_services.dialog_service import DialogService
from pydantic import BaseModel
from db.settings import retrievaler
import json
import threading
from copy import deepcopy
from datetime import datetime
from agent.agent_utils import contact_info

class Parameters(BaseModel):
    pass

class RetrievalParameters(BaseModel):
    query: str


def decorate_answer(answer, kbinfos, embd_mdl, vector_similarity_weight):
    answer, idx = retrievaler.insert_citations(answer, [ck["content_ltks"] for ck in kbinfos["chunks"]],
                                                [ck["vector"] for ck in kbinfos["chunks"]],
                                                embd_mdl=embd_mdl,
                                                tkweight=1 - vector_similarity_weight,
                                                vtweight=vector_similarity_weight)
    idx = set([kbinfos["chunks"][int(i)]["doc_id"] for i in idx])
    recall_docs = [d for d in kbinfos["doc_aggs"] if d["doc_id"] in idx]
    if not recall_docs:
        recall_docs = kbinfos["doc_aggs"]
    kbinfos["doc_aggs"] = recall_docs
    refs = deepcopy(kbinfos)
    for c in refs["chunks"]:
        if c.get("vector"):
            del c["vector"]
    if answer.lower().find("invalid key") >= 0 or answer.lower().find("invalid api") >= 0:
        answer += " Please set LLM API-Key in 'User Setting -> Model Providers -> API-Key'"
    return {"answer": answer, "reference": refs}      

def rerank_results(query, ranks, dialog, top_n=4, rerank_mdl=None):
    if not ranks["chunks"]:
        return ranks
    if not rerank_mdl:
        sim, tsim, vsim = retrievaler.manual_rerank(dialog.id, [ck["content_with_weight"] for ck in ranks["chunks"]], query)
    else:
        sim, tsim, vsim = retrievaler.manual_rerank_by_model(rerank_mdl, [ck["content_with_weight"] for ck in ranks["chunks"]], query,
                                                                 1 - dialog.vector_similarity_weight, dialog.vector_similarity_weight)

    chunks = [chunk for _, chunk in sorted(zip(sim, [ck for ck in ranks["chunks"]]), key=lambda pair: pair[0], reverse=True)]
    ranks["chunks"] = chunks[:top_n]
    for ck in ranks["chunks"]:
        dnm = ck["docnm_kwd"]
        did = ck["doc_id"]
        if dnm not in ranks["doc_aggs"]:
            ranks["doc_aggs"][dnm] = {"doc_id": did, "count": 0}
        ranks["doc_aggs"][dnm]["count"] += 1
    ranks["doc_aggs"] = [{"doc_name": k,
                            "doc_id": v["doc_id"],
                            "count": v["count"]} for k,
                            v in sorted(ranks["doc_aggs"].items(),
                                    key=lambda x:x[1]["count"] * -1)]
    return ranks

def merged_results(ranks_1, ranks_2):
    chunks = ranks_1["chunks"] + ranks_2["chunks"]
    ranks = {"total": ranks_1["total"] + ranks_2["total"], "chunks": chunks, "doc_aggs": {}}
    for ck in chunks:
        dnm = ck["docnm_kwd"]
        did = ck["doc_id"]
        if dnm not in ranks["doc_aggs"]:
            ranks["doc_aggs"][dnm] = {"doc_id": did, "count": 0}
        ranks["doc_aggs"][dnm]["count"] += 1
    ranks["doc_aggs"] = [{"doc_name": k,
                            "doc_id": v["doc_id"],
                            "count": v["count"]} for k,
                            v in sorted(ranks["doc_aggs"].items(),
                                    key=lambda x:x[1]["count"] * -1)]
    return ranks

def decorate_answer(answer, kbinfos, embd_mdl, vector_similarity_weight):
    answer, idx = retrievaler.insert_citations(answer, [ck["content_ltks"] for ck in kbinfos["chunks"]],
                                                [ck["vector"] for ck in kbinfos["chunks"]],
                                                embd_mdl=embd_mdl,
                                                tkweight=1 - vector_similarity_weight,
                                                vtweight=vector_similarity_weight)
    idx = set([kbinfos["chunks"][int(i)]["doc_id"] for i in idx])
    recall_docs = [d for d in kbinfos["doc_aggs"] if d["doc_id"] in idx]
    if not recall_docs:
        recall_docs = kbinfos["doc_aggs"]
    kbinfos["doc_aggs"] = recall_docs
    refs = deepcopy(kbinfos)
    for c in refs["chunks"]:
        if c.get("vector"):
            del c["vector"]
    if answer.lower().find("invalid key") >= 0 or answer.lower().find("invalid api") >= 0:
        answer += " Please set LLM API-Key in 'User Setting -> Model Providers -> API-Key'"
    return {"answer": answer, "reference": refs}        

class AgentWorkFlow:
    def __init__(self, dialog):
        model_config = TenantLLMService.get_api_key(dialog.tenant_id, dialog.llm_id)
        if model_config:
            model_config = model_config.to_dict()
        else:
            raise ValueError("No model config found")
        self.chat_mdl = OpenAI(api_key=model_config["api_key"],
                                     base_url=model_config["api_base"])
        self.rerank_mdl = None
        self.chat_mdl_name = model_config["llm_name"]
        if dialog.rerank_id:
            self.rerank_mdl = LLMBundle(dialog.tenant_id, LLMType.RERANK, dialog.rerank_id)
        self.kbs = KnowledgebaseService.get_by_ids(dialog.kb_ids)
        self.dialog= dialog
        embd_nms = list(set([kb.embd_id for kb in self.kbs]))
        if len(embd_nms) != 1:
            raise ValueError("The number of embd_nms is not 1")
        self.embd_mdl = LLMBundle(dialog.tenant_id, LLMType.EMBEDDING, embd_nms[0])
        self.tools = {
            "next": openai.pydantic_function_tool(Parameters, name="next", description="Get next step"),
            "retrieval": openai.pydantic_function_tool(RetrievalParameters, name="retrieval", description="Retrieval knowledge")
        }
        
    def hello(self, query: str):
        system_prompt = """
B·∫°n l√† tr·ª£ l√Ω ·∫£o ch√≠nh th·ª©c c·ªßa Tr∆∞·ªùng ƒê·∫°i h·ªçc C√¥ng ngh·ªá - ƒêHQGHN. H√£y th·ª±c hi·ªán c√°c b∆∞·ªõc sau:

1. Ph√¢n lo·∫°i c√¢u h·ªèi:
- N·∫øu l√† ch√†o h·ªèi/x√£ giao (VD: "Xin ch√†o", "Hello", "Ch√†o bu·ªïi s√°ng"): 
   ‚Üí Ch√†o l·∫°i l·ªãch s·ª± v√† m·ªùi ƒë·∫∑t c√¢u h·ªèi (VD: "Xin ch√†o! T√¥i c√≥ th·ªÉ gi√∫p g√¨ cho b·∫°n v·ªÅ th√¥ng tin ƒê·∫°i h·ªçc C√¥ng ngh·ªá?")
- N·∫øu l√† c√¢u h·ªèi chuy√™n m√¥n/tr∆∞·ªùng h·ªçc ho·∫∑c nh·ªØng c√¢u h·ªèi t·ªëi nghƒ©a kh√°c: 
   ‚Üí G·ªçi tool "next" ƒë·ªÉ chuy·ªÉn sang giai ƒëo·∫°n ph√¢n t√≠ch

2. Nguy√™n t·∫Øc:
- Gi·ªØ ph·∫£n h·ªìi ng·∫Øn g·ªçn (<2 c√¢u) v·ªõi c√¢u x√£ giao
- Lu√¥n d√πng ti·∫øng Vi·ªát ph·ªï th√¥ng
- Kh√¥ng t·ª± tr·∫£ l·ªùi c√¢u h·ªèi chuy√™n m√¥n khi ch∆∞a qua tool
- G·ª£i √Ω c√°c ch·ªß ƒë·ªÅ th∆∞·ªùng g·∫∑p n·∫øu c·∫ßn (tuy·ªÉn sinh, ch∆∞∆°ng tr√¨nh ƒë√†o t·∫°o, h·ªçc ph√≠)
"""
        response = self.chat_mdl.chat.completions.create(
            model=self.chat_mdl_name,
            messages=[
                {"role": "system", "content": dedent(system_prompt)},
                {"role": "user", "content": query}
            ],
            tools=[self.tools["next"]],
            stream=True
        )
        tools = []
        for chunk in response:
            if chunk.choices[0].delta.tool_calls:
                tool_calls = chunk.choices[0].delta.tool_calls
                if len(tools) == 0:
                    tools = [{"name": tool_call.function.name, "arguments": tool_call.function.arguments} for tool_call in tool_calls]
                else:
                    for idx, tool_call in enumerate(tool_calls):
                        tools[idx]["arguments"] += tool_call.function.arguments
            elif not tools and chunk.choices[0].delta.content:
                yield chunk.choices[0].delta.content
        if tools:
            yield tools
    
    def analyze(self, query: str, history: list[object]):
        time = datetime.now().date()
        system_prompt = """
        
Th·ªùi gian hi·ªán t·∫°i l√†: {time}
B·∫°n l√† chuy√™n gia ph√¢n t√≠ch c√¢u h·ªèi h·ªçc thu·∫≠t. H√£y ph√¢n t√≠ch c√¢u h·ªèi theo c·∫•u tr√∫c:

1. X√°c ƒë·ªãnh ch·ªß ƒë·ªÅ ch√≠nh: 
   - Lƒ©nh v·ª±c (tuy·ªÉn sinh, ƒë√†o t·∫°o, nghi√™n c·ª©u...)
   - ƒê·ªëi t∆∞·ª£ng ƒë∆∞·ª£c h·ªèi (sinh vi√™n, th·∫°c sƒ©, nghi√™n c·ª©u sinh)

2. Ph√¢n r√£ th√†nh c√°c y·∫øu t·ªë c·∫ßn truy xu·∫•t:
   - T·ª´ kh√≥a ch√≠nh (VD: "h·ªçc ph√≠", "ch·ªâ ti√™u", "ng√†nh CNTT")
   - Th√¥ng tin b·ªï tr·ª£ (th·ªùi gian, ƒë·ªãa ƒëi·ªÉm, ƒëi·ªÅu ki·ªán k√®m theo)

3. T√≥m t·∫Øt truy v·∫•n:
   - T·∫°o 1-3 truy v·∫•n con ng·∫Øn g·ªçn (<15 t·ª´) t·∫≠p trung v√†o y·∫øu t·ªë ch√≠nh
   - ∆Øu ti√™n danh t·ª´ ri√™ng, thu·∫≠t ng·ªØ chuy√™n m√¥n

V√≠ d·ª•:
C√¢u h·ªèi: "ƒêi·ªÅu ki·ªán x√©t tuy·ªÉn ng√†nh Khoa h·ªçc M√°y t√≠nh nƒÉm 2024 g·ªìm nh·ªØng g√¨?"
‚Üí Truy v·∫•n: ["ƒêi·ªÅu ki·ªán x√©t tuy·ªÉn 2024", "Ng√†nh Khoa h·ªçc M√°y t√≠nh"]
"""
        response = self.chat_mdl.chat.completions.create(
            model=self.chat_mdl_name,
            messages=[
                *history,
                {"role": "system", "content": dedent(system_prompt.format(time=time))},
                {"role": "user", "content": query}
            ],
    
            stream=True
        )
        for chunk in response:
            if chunk.choices[0].delta.content:
                yield chunk.choices[0].delta.content


    def analyze_tool_call(self, query: str, guidance: str):
        system_prompt = """
B·∫°n l√† h·ªá th·ªëng x·ª≠ l√Ω truy v·∫•n th√¥ng minh. H√£y:

1. Ph√¢n t√≠ch c√°c y·∫øu t·ªë t·ª´ ph·∫ßn t∆∞ v·∫•n vi√™n:
- C√°c ch·ªß ƒë·ªÅ con c·∫ßn truy xu·∫•t
- T·ª´ kh√≥a then ch·ªët v√† t·ª´ ƒë·ªìng nghƒ©a

2. T·∫°o truy v·∫•n hi·ªáu qu·∫£:
- T·ªëi ƒëa 3 truy v·∫•n/c√¢u h·ªèi
- K·∫øt h·ª£p t·ª´ kh√≥a theo c√∫ ph√°p: [A] [B] site:uet.vnu.edu.vn 
- Lo·∫°i b·ªè t·ª´ kh√¥ng c·∫ßn thi·∫øt (t·ª´ h·ªèi, t·ª´ n·ªëi)

V√≠ d·ª•:
Ph√¢n t√≠ch: "Th√¥ng tin tuy·ªÉn sinh h·ªá th·∫°c sƒ© 2024"
‚Üí Truy v·∫•n: ["th√¥ng tin tuy·ªÉn sinh th·∫°c sƒ© 2024 site:uet.vnu.edu.vn"]

H∆∞·ªõng d·∫´n t·ª´ t∆∞ v·∫•n vi√™n:
{guidance}
"""
        response=  self.chat_mdl.chat.completions.create(
            model=self.chat_mdl_name,
            messages=[
                {"role": "system", "content": dedent(system_prompt.format(guidance=guidance))},
                {"role": "user", "content": query}
            ],
            tools=[self.tools["retrieval"]],
            tool_choice="required"
        )
        tool_calls = []
        for tool_call in response.choices[0].message.tool_calls:
            tool_calls.append({"name": tool_call.function.name, "query": json.loads(tool_call.function.arguments)["query"]})
        return tool_calls
        
    def answer(self, query: str, knowledge: str, history):
        system_prompt = """B·∫°n l√† chuy√™n gia t∆∞ v·∫•n c·ªßa ƒê·∫°i h·ªçc C√¥ng ngh·ªá. H√£y tr·∫£ l·ªùi c√¢u h·ªèi theo c√°c b∆∞·ªõc:

1. T√ìM T·∫ÆT TRI TH·ª®C:
- Li·ªát k√™ c√°c th√¥ng tin li√™n quan t·ª´ ngu·ªìn d·ªØ li·ªáu
- Ghi ch√∫ c√°c con s·ªë, m·ªëc th·ªùi gian, ƒëi·ªÅu ki·ªán quan tr·ªçng

2. PH√ÇN T√çCH Y√äU C·∫¶U:
- ƒê·ªëi chi·∫øu v·ªõi t·ª´ng ph·∫ßn c·ªßa c√¢u h·ªèi
- X√°c ƒë·ªãnh th√¥ng tin ƒë·ªß/thi·∫øu c·∫ßn l√†m r√µ

3. TR·∫¢ L·ªúI:
- Tr√¨nh b√†y theo c·∫•u tr√∫c: Gi·ªõi thi·ªáu ‚Üí Chi ti·∫øt ‚Üí K·∫øt lu·∫≠n
- ƒê√°nh s·ªë c√°c ƒëi·ªÅu ki·ªán/y√™u c·∫ßu quan tr·ªçng

QUY T·∫ÆC:
- Ch·ªâ s·ª≠ d·ª•ng th√¥ng tin t·ª´ knowledge base
- Gi·ªØ c√¢u tr·∫£ l·ªùi d∆∞·ªõi 500 t·ª´
- D√πng ng√¥n ng·ªØ t·ª± nhi√™n, kh√¥ng d·∫°ng bullet point
- N·∫øu thi·∫øu th√¥ng tin: Y√™u c·∫ßu l√†m r√µ ho·∫∑c cung c·∫•p li√™n h·ªá b·ªô ph·∫≠n chuy√™n m√¥n

Knowledge base:
{knowledge}
"""

        response = self.chat_mdl.chat.completions.create(
            model=self.chat_mdl_name,
            messages=[
                *history,
                {"role": "system", "content": dedent(system_prompt.format(knowledge=knowledge))},
                {"role": "user", "content": query}
            ],
            stream=True
        )
        for chunk in response:
            if chunk.choices[0].delta.content:
                yield chunk.choices[0].delta.content
                
        yield "\n\n" + contact_info
    
    def retrieval(self, questions: list[object], top_k=None):
        threads = []
        lock = threading.Lock()
        ranks = {"total": 0, "chunks": [], "doc_aggs": {}}
        top_n = top_k if top_k else self.dialog.top_n
        def retrie(query: str, external_source=False):
            kbinfos = retrievaler.retrieval(
                query, self.embd_mdl, self.dialog.tenant_id, self.dialog.kb_ids, 1, top_n,
                self.dialog.similarity_threshold, self.dialog.vector_similarity_weight,
                doc_ids=None, top=self.dialog.top_k, aggs=False, rerank_mdl=None, external_source=external_source
            )
            with lock:
                ranks["total"] += kbinfos["total"]
                for ck in kbinfos["chunks"]:
                    if ck["chunk_id"] not in [c["chunk_id"] for c in ranks["chunks"]]:
                        ranks["chunks"].append(ck)
        for question in questions:
            thread = threading.Thread(target=retrie, args=(question["query"], question["external_source"]))
            threads.append(thread)
            thread.start()
        for thread in threads:
            thread.join()
        return ranks
    
    def _run(self, query: str, history: list[object]):
        tools = []
        ans = ""
        think_msg = ""
        for chunk in self.hello(query):
            if type(chunk) == list:
                tools = chunk
                yield {
                    "state_msg": "üïí Xin h√£y ƒë·ª£i ch√∫t, t√¥i ƒëang ph√¢n t√≠ch c√¢u h·ªèi...",
                    "response": {
                        "answer": ans,
                        "reference": {}
                    },
                    "think": think_msg
                }
                break
            ans += chunk
            yield {
                "state_msg": "",
                "response": {
                    "answer": ans,
                    "reference": {}
                },
                "think": think_msg 
            }
        if not tools:
            return
        guidance = ""
        for chunk in self.analyze(query, history):
            guidance += chunk
            think_msg += chunk
            yield {
                "state_msg": "",
                "response": {
                    "answer": ans,
                    "reference": {}
                },
                "think": think_msg
            }
        tools = self.analyze_tool_call(query, guidance)
        if tools:
            yield {
                "state_msg": "üìö ƒêang t√¨m ki·∫øm d·ªØ li·ªáu trong kho ki·∫øn th·ª©c",
                "response": {
                    "answer": "",
                    "reference": {}
                },
                "think": think_msg
            }
            questions = []
            merged_query = query
            for tool in tools:
                if tool["name"] == "retrieval":
                    merged_query += "\n" + tool["query"]
                    questions.append({
                        "query": tool["query"],
                        "external_source": False
                    })
#             yield """T√¥i s·∫Ω t√¨m ki·∫øm nh·ªØng th√¥ng tin li√™n quan ƒë·∫øn c√°c c√¢u h·ªèi sau:
# - {questions}""".format(questions="\n- ".join(questions))
            ranks = self.retrieval(questions)
            external_source_ranks = self.retrieval([{
                "query": query,
                "external_source": False
            }], 6)
            ranks = rerank_results(merged_query, deepcopy(ranks), self.dialog, 10, self.rerank_mdl)
            knowledges = [ck["content_with_weight"] for ck in ranks["chunks"]]
            for chunk in self.answer(query, "\n\n\n".join(knowledges), history):
                ans += chunk
                yield {
                    "state_msg": "",
                    "response": {
                        "answer": ans,
                        "reference": {}
                    },
                    "think": think_msg
                }
            merged_ranks = merged_results(ranks, external_source_ranks)
            answer = decorate_answer(ans, merged_ranks, self.embd_mdl, self.dialog.vector_similarity_weight)
            yield {
                "state_msg": "",
                "response": answer,
                "think": think_msg
            }
            
                
def clean_text(text: str) -> str:
    import unicodedata
    # C√°ch 1: Normalize v·ªÅ d·∫°ng NFKC
    normalized = unicodedata.normalize("NFKC", text)
    # C√°ch 2: N·∫øu v·∫´n c√≥ l·ªói, b·∫°n c√≥ th·ªÉ thay th·∫ø c√°c k√Ω t·ª± kh√¥ng encode ƒë∆∞·ª£c:
    cleaned = normalized.encode("utf-8", "replace").decode("utf-8")
    return cleaned
            
if __name__ == '__main__':
    question = input("Question: ")
    question = clean_text(question)
    dialog_id = "8eff7bbed70a11ef86200242ac120006"
    e, dialog = DialogService.get_by_id(dialog_id)
    if not e:
        raise ValueError("Dialog not found")
    agent = AgentWorkFlow(dialog)
    think = ""
    answer = ""
    history = [
    ]
    for ans in agent._run(question, history):
        if ans["state_msg"]:
            state_msg = ans["state_msg"]
            print(f"\n\n-------------{state_msg}-------------\n\n")
        if ans["response"]["answer"] \
            and answer != ans["response"]["answer"]:
            if not answer:
                next_token = ans["response"]["answer"]
            else: 
                next_token = ans["response"]["answer"].split(answer)[-1]
            answer = ans["response"]["answer"]
            print(next_token, end="")
        if ans["think"] and think != ans["think"]:
            if not think:
                next_token = ans["think"]
            else:
                next_token = ans["think"].split(think)[-1]
            think = ans["think"]
            print(next_token, end="")