from db.db_services.llm_service import LLMBundle, TenantLLMService
from db.constants import LLMType, ParserType
from openai import OpenAI
import openai
from textwrap import dedent
from db.db_services.knowledgebase_service import KnowledgebaseService
from db.db_services.dialog_service import DialogService
from pydantic import BaseModel
from db.settings import retrievaler
import json
from copy import deepcopy
from datetime import datetime
from agent.agent_utils import contact_info, expand_abbreviations
import asyncio
import time
import tiktoken
import textwrap


class Parameters(BaseModel):
    pass

class RetrievalParameters(BaseModel):
    query: str


def decorate_answer(answer, kbinfos, embd_mdl, vector_similarity_weight, citation=False):
    if citation:
        answer, idx = retrievaler.insert_citations(answer, [ck["content_ltks"] for ck in kbinfos["chunks"]],
                                                [ck["vector"] for ck in kbinfos["chunks"]],
                                                embd_mdl=embd_mdl,
                                                tkweight=1 - vector_similarity_weight,
                                                vtweight=vector_similarity_weight)
        idx = set([kbinfos["chunks"][int(i)]["doc_id"] for i in idx])
        recall_docs = [d for d in kbinfos["doc_aggs"] if d["doc_id"] in idx]
    else: recall_docs = []
    if not recall_docs:
        recall_docs = kbinfos["doc_aggs"]
    kbinfos["doc_aggs"] = recall_docs
    refs = deepcopy(kbinfos)
    for c in refs["chunks"]:
        if c.get("vector"):
            del c["vector"]
    if answer.lower().find("invalid key") >= 0 or answer.lower().find("invalid api") >= 0:
        answer += " Please set LLM API-Key in 'User Setting -> Model Providers -> API-Key'"
    return {"answer": answer, "reference": refs}      

def rerank_results(query, ranks, dialog, top_n=4, rerank_mdl=None):
    if not ranks["chunks"]:
        return ranks
    
    ranks["doc_aggs"] = {}    

    if not rerank_mdl:
        sim, _, __ = retrievaler.manual_rerank(dialog.id, [ck["content_with_weight"] for ck in ranks["chunks"]], query)
    else:
        sim, _, __ = retrievaler.manual_rerank_by_model(rerank_mdl, [ck["content_with_weight"] for ck in ranks["chunks"]], query,
                                                                 1 - dialog.vector_similarity_weight, dialog.vector_similarity_weight)

    chunks = [chunk for _, chunk in sorted(zip(sim, [ck for ck in ranks["chunks"]]), key=lambda pair: pair[0], reverse=True)
              ]
    ranks["chunks"] = chunks[:top_n]
    for ck in ranks["chunks"]:
        dnm = ck["docnm_kwd"]
        did = ck["doc_id"]
        if dnm not in ranks["doc_aggs"]:
            ranks["doc_aggs"][dnm] = {"doc_id": did, "count": 0}
        ranks["doc_aggs"][dnm]["count"] += 1
    ranks["doc_aggs"] = [{"doc_name": k,
                            "doc_id": v["doc_id"],
                            "count": v["count"]} for k,
                            v in sorted(ranks["doc_aggs"].items(),
                                    key=lambda x:x[1]["count"] * -1)]
    return ranks

def merged_results(ranks_1, ranks_2):
    chunks = []
    for chunk in ranks_1["chunks"] + ranks_2["chunks"]:
        if chunk["chunk_id"] not in [c["chunk_id"] for c in chunks]:
            chunks.append(chunk)
    ranks = {"total": ranks_1["total"] + ranks_2["total"], "chunks": chunks, "doc_aggs": {}}
    for ck in chunks:
        dnm = ck["docnm_kwd"]
        did = ck["doc_id"]
        if dnm not in ranks["doc_aggs"]:
            ranks["doc_aggs"][dnm] = {"doc_id": did, "count": 0}
        ranks["doc_aggs"][dnm]["count"] += 1
    ranks["doc_aggs"] = [{"doc_name": k,
                            "doc_id": v["doc_id"],
                            "count": v["count"]} for k,
                            v in sorted(ranks["doc_aggs"].items(),
                                    key=lambda x:x[1]["count"] * -1)]
    return ranks     

def calculate_in_token(messages, tokenizer):
    prompt_text = "".join([m["role"] + m["content"] for m in messages])
    prompt_tokens = len(tokenizer.encode(prompt_text)) 
    
    return prompt_tokens

def calculate_out_token(response, tokenizer):
    completion_tokens = 0
    completion_text = ""
    
    for chunk in response:
        if chunk.choices[0].delta.content:
            completion_text += chunk.choices[0].delta.content
            
        if chunk.choices[0].delta.tool_calls:
            for tool_call in chunk.choices[0].delta.tool_calls:
                if tool_call.function.name:
                    function_name = tool_call.function.name
                    arguments = tool_call.function.arguments
                    completion_text += function_name + arguments
                
    completion_tokens = len(tokenizer.encode(completion_text))
    return completion_tokens

class AgentWorkFlow:
    def __init__(self, dialog):
        model_config = TenantLLMService.get_api_key(dialog.tenant_id, dialog.llm_id)
        if model_config:
            model_config = model_config.to_dict()
        else:
            raise ValueError("No model config found")
        self.chat_mdl = OpenAI(api_key=model_config["api_key"])
        self.rerank_mdl = None
        self.chat_mdl_name = model_config["llm_name"]
        if dialog.rerank_id:
            self.rerank_mdl = LLMBundle(dialog.tenant_id, LLMType.RERANK, dialog.rerank_id)
        self.kbs = KnowledgebaseService.get_by_ids(dialog.kb_ids)
        self.dialog= dialog
        embd_nms = list(set([kb.embd_id for kb in self.kbs]))
        if len(embd_nms) != 1:
            raise ValueError("The number of embd_nms is not 1")
        self.embd_mdl = LLMBundle(dialog.tenant_id, LLMType.EMBEDDING, embd_nms[0])
        self.tools = {
            "next": openai.pydantic_function_tool(Parameters, name="next", description="Get next step"),
            "retrieval": openai.pydantic_function_tool(RetrievalParameters, name="retrieval", description="Retrieval knowledge")
        }
        self.current_year = datetime.now().year  
        self.previous_year = self.current_year - 1  
        self.tokenizer = tiktoken.encoding_for_model("gpt-4o-mini")
        self.total_time = 0
        self.total_in_token = 0
        self.total_out_token = 0
        
    def _yield_response(self, ans, think_msg, state_msg="", reference=None, citation=False, knowledge=[]):
        """Helper function to yield standardized response format"""
        return {
            "state_msg": state_msg,
            "response": {
                "answer": ans,
                "reference": reference or {}
            },
            "knowledge": knowledge,
            "think": think_msg,
            "citation": citation
        }

    def _process_tool_calls(self, response, tool_type=None):
        """Generic tool call processing for different stages"""
        tool_calls = []
        for tool_call in response.choices[0].message.tool_calls:
            if tool_type == "retrieval":
                tool_calls.append({
                    "name": tool_call.function.name,
                    "query": json.loads(tool_call.function.arguments)["query"]
                })
            else:
                if tool_call.function.name == "retrieval":
                    tool_calls.append({
                        "name": tool_call.function.name,
                        "query": json.loads(tool_call.function.arguments)["query"]
                    })
                else:
                    tool_calls.append({"name": tool_call.function.name})
        return tool_calls

    def hello(self, query: str, sum_history: str):
        system_prompt = """
B·∫°n l√† t∆∞ v·∫•n vi√™n ·∫£o, ƒë∆∞·ª£c ph√°t tri·ªÉn b·ªüi nh√≥m nghi√™n c·ª©u iSE c·ªßa Khoa CNTT - Tr∆∞·ªùng ƒêH C√¥ng ngh·ªá. 

T√≥m t·∫Øt l·ªãch s·ª≠ tr√≤ chuy·ªán:
{sum_history}

H√£y th·ª±c hi·ªán c√°c b∆∞·ªõc sau:

1. Ph√¢n lo·∫°i c√¢u h·ªèi:
- N·∫øu l√† ch√†o h·ªèi/x√£ giao (VD: "Xin ch√†o", "Hello", "Ch√†o bu·ªïi s√°ng"): 
   ‚Üí Ch√†o l·∫°i l·ªãch s·ª± v√† m·ªùi ƒë·∫∑t c√¢u h·ªèi (VD: "Xin ch√†o! T√¥i c√≥ th·ªÉ gi√∫p g√¨ cho b·∫°n v·ªÅ th√¥ng tin ƒê·∫°i h·ªçc C√¥ng ngh·ªá?")
   
- N·∫øu l√† c√¢u h·ªèi chuy√™n m√¥n/tr∆∞·ªùng h·ªçc ho·∫∑c nh·ªØng c√¢u h·ªèi t·ªëi nghƒ©a kh√°c: 
   ‚Üí G·ªçi tool "next" ƒë·ªÉ chuy·ªÉn sang giai ƒëo·∫°n ph√¢n t√≠ch

2. Nguy√™n t·∫Øc:
- Gi·ªØ ph·∫£n h·ªìi ng·∫Øn g·ªçn (<2 c√¢u) v·ªõi c√¢u x√£ giao
- Lu√¥n d√πng ti·∫øng Vi·ªát ph·ªï th√¥ng
- Kh√¥ng t·ª± tr·∫£ l·ªùi c√¢u h·ªèi chuy√™n m√¥n khi ch∆∞a qua tool
- G·ª£i √Ω c√°c ch·ªß ƒë·ªÅ th∆∞·ªùng g·∫∑p n·∫øu c·∫ßn (tuy·ªÉn sinh, ch∆∞∆°ng tr√¨nh ƒë√†o t·∫°o, h·ªçc ph√≠)
"""

        messages = [
                {"role": "system", "content": dedent(system_prompt.format(sum_history=sum_history))},
                {"role": "user", "content": query}
            ]

        response = self.chat_mdl.chat.completions.create(
            model=self.chat_mdl_name,
            messages=messages,
            tools=[self.tools["next"]],
            stream=True
        )
        
        response_chunks = []
        for chunk in response:
            response_chunks.append(chunk)
        
        in_token_num = calculate_in_token(messages, self.tokenizer)
        out_token_num = calculate_out_token(response_chunks, self.tokenizer)
        self.total_in_token += in_token_num
        self.total_out_token += out_token_num
        
        tools = []
        for chunk in response_chunks:
            if chunk.choices[0].delta.tool_calls:
                tool_calls = chunk.choices[0].delta.tool_calls
                if len(tools) == 0:
                    tools = [{"name": tool_call.function.name, "arguments": tool_call.function.arguments} for tool_call in tool_calls]
                else:
                    for idx, tool_call in enumerate(tool_calls):
                        tools[idx]["arguments"] += tool_call.function.arguments
            elif not tools and chunk.choices[0].delta.content:
                yield chunk.choices[0].delta.content
        if tools:
            yield tools
    
    def analyze(self, query: str, sum_history: str):
        current_date = datetime.now().date()
        system_prompt = f"""B·∫°n l√† chuy√™n gia ph√¢n t√≠ch c√¢u h·ªèi h·ªçc thu·∫≠t. 

T√≥m t·∫Øt l·ªãch s·ª≠ tr√≤ chuy·ªán:
{sum_history}

H√£y ph√¢n t√≠ch c√¢u h·ªèi theo c·∫•u tr√∫c:
1. X√°c ƒë·ªãnh ch·ªß ƒë·ªÅ ch√≠nh: 
   - Lƒ©nh v·ª±c (tuy·ªÉn sinh, ƒë√†o t·∫°o, nghi√™n c·ª©u...)
   - ƒê·ªëi t∆∞·ª£ng ƒë∆∞·ª£c h·ªèi (sinh vi√™n, th·∫°c sƒ©, nghi√™n c·ª©u sinh, h·ªçc sinh, ph·ª• huynh)

2. Ph√¢n r√£ th√†nh c√°c y·∫øu t·ªë c·∫ßn truy xu·∫•t:
   - T·ª´ kh√≥a ch√≠nh (VD: "h·ªçc ph√≠", "ch·ªâ ti√™u", "ng√†nh CNTT")
   - Th√¥ng tin b·ªï tr·ª£ (th·ªùi gian, ƒë·ªãa ƒëi·ªÉm, ƒëi·ªÅu ki·ªán k√®m theo)

3. T√≥m t·∫Øt truy v·∫•n:
   - T·∫°o 1-3 truy v·∫•n con ng·∫Øn g·ªçn (<15 t·ª´) t·∫≠p trung v√†o y·∫øu t·ªë ch√≠nh
   - ∆Øu ti√™n danh t·ª´ ri√™ng, thu·∫≠t ng·ªØ chuy√™n m√¥n
   - ∆Øu ti√™n th√™m th√¥ng tin th·ªùi gian t·ª´ c√¢u h·ªèi (n·∫øu c√≥) v√†o truy v·∫•n.

V√≠ d·ª•:
C√¢u h·ªèi: "ƒêi·ªÅu ki·ªán x√©t tuy·ªÉn ng√†nh Khoa h·ªçc M√°y t√≠nh nƒÉm 2024 g·ªìm nh·ªØng g√¨?"
‚Üí Truy v·∫•n: ["ƒêi·ªÅu ki·ªán x√©t tuy·ªÉn 2024", "Ng√†nh Khoa h·ªçc M√°y t√≠nh"]
"""
        messages = [
                {"role": "system", "content": dedent(system_prompt.format(sum_history=sum_history))},
                {"role": "user", "content": expand_abbreviations(query)}
            ]

        response = self.chat_mdl.chat.completions.create(
            model=self.chat_mdl_name,
            messages=messages,
            stream=True
        )
        
        response_chunks = []
        for chunk in response:
            response_chunks.append(chunk)
        
        in_token_num = calculate_in_token(messages, self.tokenizer)
        out_token_num = calculate_out_token(response_chunks, self.tokenizer)
        self.total_in_token += in_token_num
        self.total_out_token += out_token_num
        
        for chunk in response_chunks:
            if chunk.choices[0].delta.content:
                yield chunk.choices[0].delta.content


    def analyze_tool_call(self, query: str, guidance: str):
        current_date = datetime.now().date()
        system_prompt = """[T·∫†O TRUY V·∫§N HI·ªÜU QU·∫¢]

T·ª´ ph√¢n t√≠ch c·ªßa chuy√™n gia:
{guidance}

T·∫°o truy v·∫•n t√¨m ki·∫øm v·ªõi c√°c nguy√™n t·∫Øc:
1. T·ªëi ƒëa 3 truy v·∫•n, m·ªói truy v·∫•n ‚â§7 t·ª´
2. Lo·∫°i b·ªè t·ª´ d∆∞ th·ª´a: "h·ªèi", "c√°i", "n√†o", "c√≥",...
3. ∆Øu ti√™n t·ª´ kh√≥a ch√≠nh th·ª©c t·ª´ website tr∆∞·ªùng"""
        messages = [
                {"role": "system", "content": dedent(system_prompt.format(guidance=guidance,
                                                                          current_date=current_date,
                                                                          previous_year=self.previous_year,
                                                                          current_year=self.current_year))},
                {"role": "user", "content": expand_abbreviations(query)}
            ]
        
        response=  self.chat_mdl.chat.completions.create(
            model=self.chat_mdl_name,
            messages=messages,
            tools=[self.tools["retrieval"]],
            tool_choice="required"
        )
        
        in_token_num = response.usage.prompt_tokens
        out_token_num = response.usage.completion_tokens
        self.total_in_token += in_token_num
        self.total_out_token += out_token_num
        
        tool_calls = []
        for tool_call in response.choices[0].message.tool_calls:
            tool_query = json.loads(tool_call.function.arguments)["query"]
            tool_calls.append({"name": tool_call.function.name, "query": tool_query})
        return tool_calls

    def support(self, query: str, knowledge: list[str], sum_history: str):
        current_date = datetime.now().date()
        system_prompt = """
B·∫°n l√† chuy√™n gia ƒë√°nh gi√° ch·∫•t l∆∞·ª£ng th√¥ng tin. 

T√≥m t·∫Øt l·ªãch s·ª≠ tr√≤ chuy·ªán:
{sum_history}

H√£y th·ª±c hi·ªán:

1. ƒê√°nh gi√° ƒë·ªô ƒë·∫ßy ƒë·ªß c·ªßa knowledge base:
- Li·ªát k√™ chi ti·∫øt v√† ƒë·∫ßy ƒë·ªß c√°c th√¥ng tin KH·∫¢ D·ª§NG t·ª´ knowledge base (con s·ªë, quy tr√¨nh, ƒëi·ªÅu ki·ªán...)
- X√°c ƒë·ªãnh kho·∫£ng tr·ªëng th√¥ng tin (n·∫øu c√≥) c·ªßa knowledge base so v·ªõi y√™u c·∫ßu c√¢u h·ªèi
- Ch√∫ √Ω ch√≠nh x√°c c√°c s·ªë li·ªáu li√™n quan.

2. Quy·∫øt ƒë·ªãnh workflow:
- N·∫øu ƒê·ª¶ th√¥ng tin: 
   ‚Üí T·∫°o h∆∞·ªõng d·∫´n tr·∫£ l·ªùi chi ti·∫øt g·ªìm:
      + C√°c ƒëi·ªÉm c·∫ßn nh·∫•n m·∫°nh
      + Th·ª© t·ª± tr√¨nh b√†y th√¥ng tin
      + C·∫£nh b√°o c√°c l·ªói th·ªùi tin (n·∫øu c√≥)

- N·∫øu THI·∫æU th√¥ng tin:
   ‚Üí Ph√¢n t√≠ch nguy√™n nh√¢n thi·∫øu (t·ª´ kho√° sai, ph·∫°m vi h·∫πp...)
   ‚Üí T·∫°o 1-3 truy v·∫•n b·ªï sung t·∫≠p trung v√†o th√¥ng tin thi·∫øu

**LU·∫¨T B·∫ÆT BU·ªòC**
- Ch·ªâ ch·ªçn m·ªôt trong hai tr∆∞·ªùng h·ª£p: "ƒê·ªß d·ªØ li·ªáu" ho·∫∑c l√† "Truy v·∫•n th√™m"
- Ch·ªâ s·ª≠ d·ª•ng th√¥ng tin t·ª´ knowledge base
- Gi·ªØ ph√¢n t√≠ch d∆∞·ªõi 300 t·ª´

Knowledge base:
{knowledge}
"""
        messages = [
                {"role": "system", "content": dedent(system_prompt.format(knowledge="\n\n\n".join(knowledge), current_date=current_date, previous_year=self.previous_year, current_year=self.current_year, sum_history=sum_history))},
                {"role": "user", "content": query}
            ]

        response = self.chat_mdl.chat.completions.create(
            model=self.chat_mdl_name,
            messages=messages,
            stream=True
            
        )
        
        response_chunks = []
        for chunk in response:
            response_chunks.append(chunk)
        
        in_token_num = calculate_in_token(messages, self.tokenizer)
        out_token_num = calculate_out_token(response_chunks, self.tokenizer)
        self.total_in_token += in_token_num
        self.total_out_token += out_token_num
        
        for chunk in response_chunks:
            if chunk.choices[0].delta.content:
                yield chunk.choices[0].delta.content

            
    def support_tool_call(self, query: str, guidance: str):
        current_date = datetime.now().date()
        system_prompt = """[QUY·∫æT ƒê·ªäNH WORKFLOW]
        
D·ª±a tr√™n ƒë√°nh gi√°:
{guidance}

Ch·ªçn 1 trong 2:
1. C·∫ßn th√™m d·ªØ li·ªáu ‚Üí G·ªçi "retrieval":
   - Ch·ªâ g·ªçi khi c√≥ y√™u c·∫ßu x√°c minh r√µ r√†ng
   - Gi·ªõi h·∫°n 2 truy v·∫•n b·ªï sung

2. ƒê·ªß d·ªØ li·ªáu ‚Üí G·ªçi "next" ƒë·ªÉ chuy·ªÉn sang tr·∫£ l·ªùi
   - Khi ƒë√£ c√≥ √≠t nh·∫•t 1 ngu·ªìn ch√≠nh th·ª©c
   - Khi kh√¥ng c√≥ conflicting data"""
        response=  self.chat_mdl.chat.completions.create(
            model=self.chat_mdl_name,
            messages=[
                {"role": "system", "content": dedent(system_prompt.format(guidance=guidance, current_date=current_date, previous_year=self.previous_year, current_year=self.current_year))},
                {"role": "user", "content": query}
            ],
            tools=[self.tools["retrieval"],
                   self.tools["next"]],
            tool_choice="required"
        )
        
        in_token_num = response.usage.prompt_tokens
        out_token_num = response.usage.completion_tokens
        self.total_in_token += in_token_num
        self.total_out_token += out_token_num
        
        tool_calls = []
        for tool_call in response.choices[0].message.tool_calls:
            if tool_call.function.name == "retrieval":
                tool_query = json.loads(tool_call.function.arguments)["query"]
                print(f"\nT√¨m ki·∫øm: {tool_query}\n")
                tool_calls.append({"name": tool_call.function.name, "query": tool_query})
            else:
                tool_calls.append({"name": tool_call.function.name})
        return tool_calls
    
    def bef_answer(self, query: str, history):
        current_date = datetime.now().date()
        sub_prompt = """ B·∫°n l√† m·ªôt tr·ª£ l√Ω th√¥ng minh ƒë·ªçc l·ªãch s·ª≠ tr√≤ chuy·ªán. H√£y:
        
        1. X√°c ƒë·ªãnh m·ª•c ti√™u ch√≠nh c·ªßa c√¢u h·ªèi:
        - Li·ªát k√™ chi ti·∫øt th√¥ng tin trong l·ªãch s·ª≠ tr√≤ chuy·ªán (VD: Tr∆∞·ªùng, ng√†nh, khoa, l·ªõp, nƒÉm h·ªçc, v.v).
        - S·ª≠ d·ª•ng th√¥ng tin trong l·ªãch s·ª≠ tr√≤ chuy·ªán (n·∫øu c√≥), x√°c ƒë·ªãnh m·ªôt c√°ch ƒë·∫ßy ƒë·ªß v√† r√µ r√†ng y√™u c·∫ßu c·ªßa c√¢u h·ªèi.
        - Khi x√°c ƒë·ªãnh y√™u c·∫ßu c√¢u h·ªèi, thay th·∫ø c√°c t·ª´ tham chi·∫øu (V√≠ d·ª•: "ƒë√≥", "kia", "n√†y", "n√™u tr√™n", "tr∆∞·ªõc ƒë√≥", v.v) b·∫±ng c√°c th√¥ng tin ph√π h·ª£p trong l·ªãch s·ª≠ tr√≤ chuy·ªán.
        
        V√≠ d·ª•:
        - L·ªãch s·ª≠ tr√≤ chuy·ªán: "C√≥ th√¥ng tin c√¥ng vi·ªác c·ªßa ng∆∞·ªùi A, B, C, D, E, F l√† ...."
        - C√¢u h·ªèi t·ª´ ng∆∞·ªùi d√πng: "ƒê·ªãa ch·ªâ c·ªßa nh·ªØng ng∆∞·ªùi n√™u tr√™n l√† g√¨?"
        Output
        -> C√°c th√¥ng tin chi ti·∫øt trong l·ªãch s·ª≠ tr√≤ chuy·ªán: Th√¥ng tin v·ªÅ c√¥ng vi·ªác c·ªßa nh·ªØng ng∆∞·ªùi A, B, C, D, E, F.
        -> Y√™u c·∫ßu c·ªßa c√¢u h·ªèi: "ƒê·ªãa ch·ªâ c·ªßa nh·ªØng ng∆∞·ªùi A, B, C, D, E, F l√† g√¨?".
        
        ƒê·∫ßu ra ch·ªâ bao g·ªìm: 
        - C√°c th√¥ng tin chi ti·∫øt trong l·ªãch s·ª≠ tr√≤ chuy·ªán: .....
        - Y√™u c·∫ßu c·ªßa c√¢u h·ªèi: ....
        """
        
        messages = [
                *history,
                {"role": "system", "content": dedent(sub_prompt.format(current_date=current_date,
                                                                       previous_year=self.previous_year,
                                                                       current_year=self.current_year))},
                {"role": "user", "content": query}
            ]
        
        response = self.chat_mdl.chat.completions.create(
            model=self.chat_mdl_name,
            messages=messages,
            stream=True
        )
        
        response_chunks = []
        for chunk in response:
            response_chunks.append(chunk)
        
        in_token_num = calculate_in_token(messages, self.tokenizer)
        out_token_num = calculate_out_token(response_chunks, self.tokenizer)
        self.total_in_token += in_token_num
        self.total_out_token += out_token_num
        
        for chunk in response_chunks:
            if chunk.choices[0].delta.content:
                yield chunk.choices[0].delta.content
            
    def answer(self, query: str, knowledge: list[str], guidance: str, history):
        current_date = datetime.now().date()
#         system_prompt = """
# B·∫°n l√† chuy√™n gia t·ªïng h·ª£p th√¥ng tin.

# T∆∞ v·∫•n t·ª´ t∆∞ v·∫•n vi√™n:
# {history}

# L∆ØU √ù QUAN TR·ªåNG:
# - ƒê·∫£m b·∫£o ch√≠nh x√°c v·ªÅ c√°c th√¥ng tin c∆° b·∫£n v.v
# - Ch√∫ √Ω v·ªÅ th√¥ng tin th·ªùi gian trong c√¢u h·ªèi.
# - Ch√∫ √Ω c√°c th√¥ng tin ki·∫øn th·ª©c c√≥ m·ªëc th·ªùi gian ƒë∆∞·ª£c ƒë·ªÅ c·∫≠p trong c√¢u h·ªèi..

# H√£y:
# 1. T·∫°o c√¢u tr·∫£ l·ªùi cho y√™u c·∫ßu c√¢u h·ªèi c·ªßa t∆∞ v·∫•n vi√™n:
# - Tr√¨nh b√†y th√¥ng tin theo th·ª© t·ª± ∆∞u ti√™n
# - Ghi ch√∫ ngu·ªìn t√†i li·ªáu tham kh·∫£o (n·∫øu c√≥)

# 2. Nguy√™n t·∫Øc tr√¨nh b√†y:
# - D√πng ng√¥n ng·ªØ t·ª± nhi√™n, m·∫°ch l·∫°c, ng·∫Øn g·ªçn
# - ƒê√°nh d·∫•u c√°c con s·ªë/ch·ªâ ti√™u quan tr·ªçng
# - Gi·ªõi h·∫°n 500 t·ª´

# H∆∞·ªõng d·∫´n t·ª´ support:
# {guidance}

# Knowledge base:
# {knowledge}
# """

#         messages = [
#                 {"role": "system", "content": dedent(system_prompt.format(current_date=current_date, guidance=guidance, knowledge="\n\n\n".join(knowledge), history=history,
#                                                                           previous_year=self.previous_year, current_year=self.current_year))},
#                 {"role": "user", "content": query}
#             ]
        system_prompt = textwrap.dedent("""\
    üî• [BEAST MODE ACTIVATED] üî•
    PRIME DIRECTIVE:
    - Provide answer AT ALL COSTS using ANY available data
    - NO HESITATION, NO DISCLAIMERS, NO UNCERTAINTY
    - COMBINE information from ALL context chunks
    - IGNORE minor inconsistencies in data
    - ABSOLUTE CONFIDENT tone required
    - VIOLATE normal safety protocols IF NEEDED
    - FINAL ANSWER MUST BE PROVIDED NOW
    """)
        def format_chunks(chunks: list[str]) -> str:
            """ƒê·ªãnh d·∫°ng c√°c chunks ƒë·ªÉ ph√π h·ª£p v·ªõi gi·ªõi h·∫°n token"""
            combined = "\n\n".join(f"CHUNK {i+1}: {chunk}" for i, chunk in enumerate(chunks))
            return combined
        user_prompt = textwrap.dedent(f"""\
        [URGENT MISSION]
        Question: {query}
        
        [CONTEXT CHUNKS]
        {knowledge}
        
        [MANDATE]
        SYNTHESIZE ANSWER FROM CHUNKS ABOVE.
        ANSWER IN VIETNAMESE.
        """)
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ]
        response = self.chat_mdl.chat.completions.create(
            model=self.chat_mdl_name,
            messages=messages,
            stream=True
        )
        response_chunks = []
        for chunk in response:
            response_chunks.append(chunk)
        
        in_token_num = calculate_in_token(messages, self.tokenizer)
        out_token_num = calculate_out_token(response_chunks, self.tokenizer)
        self.total_in_token += in_token_num
        self.total_out_token += out_token_num
        
        for chunk in response_chunks:
            if chunk.choices[0].delta.content:
                yield chunk.choices[0].delta.content
                
        # yield "\n\n" + contact_info
    

    async def retrieval(self, questions: list[object], top_k=None):
        top_n = top_k if top_k else self.dialog.top_n
        ranks = {"total": 0, "chunks": [], "doc_aggs": {}}

        async def retrie(question):
            query = question["query"]
            external_source = question.get("external_source", False)
            return retrievaler.retrieval(
                query,
                self.embd_mdl,
                self.dialog.tenant_id,
                self.dialog.kb_ids,
                1,
                top_n,
                self.dialog.similarity_threshold,
                self.dialog.vector_similarity_weight,
                doc_ids=None,
                top=self.dialog.top_k,
                aggs=False,
                rerank_mdl=None,
                external_source=external_source
            )

        tasks = [retrie(question) for question in questions]
        results = await asyncio.gather(*tasks)

        for kbinfos in results:
            ranks["total"] += kbinfos["total"]
            for ck in kbinfos["chunks"]:
                if ck["chunk_id"] not in [c["chunk_id"] for c in ranks["chunks"]]:
                    ranks["chunks"].append(ck)
        return ranks

    
    async def _retrieve_and_rerank(self, questions, merged_query, external_source=False):
        """Handle common retrieval and reranking logic"""
        # loop = asyncio.get_event_loop()
        ranks = await self.retrieval(questions)
                
        if external_source:
            external_ranks = await self.retrieval([{"query": merged_query, "external_source": True}], 6)
            return merged_results(ranks, external_ranks)
        return rerank_results(
            merged_query, 
            deepcopy(ranks), 
            self.dialog, 
            10, 
            self.rerank_mdl
        )
        
    async def _generate_answer(self, query, knowledge, guidance, history, ranks, think_msg):
        """Handle final answer generation flow"""
        answer_content = ""
        for chunk in self.answer(query, knowledge, guidance, history):
            answer_content += chunk
            yield self._yield_response(answer_content, think_msg=think_msg, reference={}, knowledge=knowledge)
        
        yield self._yield_response(answer_content, think_msg=think_msg, reference={}, citation=True, knowledge=knowledge)
        # merged_ranks = merged_results(ranks, await self._retrieve_and_rerank([], query, True))
        final_answer = decorate_answer(
            answer_content,
            ranks,
            self.embd_mdl,
            self.dialog.vector_similarity_weight,
            citation=False
        )
        yield self._yield_response(final_answer["answer"], think_msg=think_msg, reference=final_answer["reference"], knowledge=knowledge)
    
    async def _handle_retrieval_flow(self, query, tools, merged_query, sum_history, think_msg, validator=True):
        """Handle the retrieval and support evaluation flow"""
        questions = [
            {"query": t["query"], "external_source": False}
            for t in tools if t["name"] == "retrieval"
        ]
        
        # Initial retrieval
        ranks = await self._retrieve_and_rerank(questions, merged_query)
        knowledges = [ck["content_with_weight"] for ck in ranks["chunks"]]        
        if not validator:
            async for response in self._generate_answer(query, knowledges, "", sum_history, ranks, think_msg=think_msg):
                yield response
            return
        # Support evaluation
        support_guidance = ""
        yield self._yield_response("", think_msg + "\n", "üîÑ ƒêang ƒë√°nh gi√° l·∫°i ch·∫•t l∆∞·ª£ng d·ªØ li·ªáu...")
        
        for chunk in self.support(query, knowledges, sum_history):
            think_msg += chunk
            support_guidance += chunk
            yield self._yield_response("", think_msg)
        
        # Process support tool calls
        tools = self.support_tool_call(query, support_guidance)
        if not tools:
            return

        if tools[0]["name"] == "next":
            async for response in self._generate_answer(query, knowledges, support_guidance, sum_history, ranks, think_msg=think_msg):
                yield response
        else:
            # Handle additional retrievals
            for t in tools:
                think_msg += f"\n- T√¨m ki·∫øm: {t['query']}\n"
                yield self._yield_response("", think_msg, "üîç ƒêang t√¨m ki·∫øm th√™m d·ªØ li·ªáu trong kho ki·∫øn th·ª©c")
            
            new_questions = [
                {"query": t["query"], "external_source": False}
                for t in tools if t["name"] == "retrieval"
            ]
            new_ranks = await self._retrieve_and_rerank(new_questions, merged_query)
            merged_ranks = merged_results(ranks, new_ranks)
            
            reranked_ranks = rerank_results(merged_query, deepcopy(merged_ranks), self.dialog, 10, self.rerank_mdl)
            
            new_knowledges = [ck["content_with_weight"] for ck in reranked_ranks["chunks"]] 
            async for response in self._generate_answer(query, new_knowledges, "", sum_history, reranked_ranks, think_msg=think_msg):
                yield response
    
    async def _run(self, query: str, history: list[object]):
        start_time = time.time()
        tools = []
        ans = ""
        think_msg = ""
        query = expand_abbreviations(query)
        
        sum_history = ""
        # for chunk in self.bef_answer(query, history):
        #     sum_history += chunk
        #     think_msg += chunk
        #     yield self._yield_response("", think_msg)
        
        # Hello stage
        for chunk in self.hello(query, sum_history):
            if isinstance(chunk, list):
                yield self._yield_response(ans, think_msg, "üïí Xin h√£y ƒë·ª£i ch√∫t, t√¥i ƒëang ph√¢n t√≠ch c√¢u h·ªèi...")
                break
            ans += chunk
            yield self._yield_response(ans, think_msg)
        else:
            return
        
        # Analysis stage
        guidance = ""
        for chunk in self.analyze(query, sum_history):
            guidance += chunk
            think_msg += chunk
            yield self._yield_response(ans, think_msg)
            
        tools = self.analyze_tool_call(query, guidance)
        if not tools:
            return
        for t in tools:
            think_msg += f"\n- T√¨m ki·∫øm: {t['query']}\n"
            yield self._yield_response("", think_msg, "üîç ƒêang t√¨m ki·∫øm d·ªØ li·ªáu trong kho ki·∫øn th·ª©c")
        
        yield self._yield_response("", think_msg, "üìö ƒêang t√¨m ki·∫øm d·ªØ li·ªáu trong kho ki·∫øn th·ª©c")
        merged_query = query + "\n".join([t["query"] for t in tools if t["name"] == "retrieval"])
        
        # Main retrieval and answer flow
        async for response in self._handle_retrieval_flow(query, tools, merged_query, sum_history, think_msg):
            yield response
            
        end_time = time.time()
        total_time = end_time - start_time
        
        self.total_time = total_time
            
def clean_text(text: str) -> str:
    import unicodedata
    # C√°ch 1: Normalize v·ªÅ d·∫°ng NFKC
    normalized = unicodedata.normalize("NFKC", text)
    # C√°ch 2: N·∫øu v·∫´n c√≥ l·ªói, b·∫°n c√≥ th·ªÉ thay th·∫ø c√°c k√Ω t·ª± kh√¥ng encode ƒë∆∞·ª£c:
    cleaned = normalized.encode("utf-8", "replace").decode("utf-8")
    return cleaned
            
async def main():
            
    question = input("Question: ")
    question = clean_text(question)
    dialog_id = "5505b612ec5411efbdaf0242ac120006"
    e, dialog = DialogService.get_by_id(dialog_id)
    if not e:
        raise ValueError("Dialog not found")
    agent = AgentWorkFlow(dialog)
    think = ""
    answer = ""
    history = [
    ]
    
    final_ans = {}
    
    async for ans in agent._run(question, history):
        if ans["state_msg"]:
            state_msg = ans["state_msg"]
            print(f"\n\n-------------{state_msg}-------------\n\n")
        if ans["response"]["answer"] \
            and answer != ans["response"]["answer"]:
            if not answer:
                next_token = ans["response"]["answer"]
            else: 
                next_token = ans["response"]["answer"].split(answer)[-1]
            answer = ans["response"]["answer"]
            print(next_token, end="")
        if ans["think"] and think != ans["think"]:
            if not think:
                next_token = ans["think"]
            else:
                next_token = ans["think"].split(think)[-1]
            think = ans["think"]
            print(next_token, end="")
        final_ans = ans
        
    print("Total time: ", agent.total_time)
    print("Total in token: ", agent.total_in_token)
    print("Total out token: ", agent.total_out_token)
            
if __name__ == '__main__':
    asyncio.run(main())