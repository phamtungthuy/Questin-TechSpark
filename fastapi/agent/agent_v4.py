from db.db_services.llm_service import LLMBundle, TenantLLMService
from db.constants import LLMType, ParserType
from openai import OpenAI
import openai
from textwrap import dedent
from db.db_services.knowledgebase_service import KnowledgebaseService
from db.db_services.dialog_service import DialogService
from pydantic import BaseModel
from db.settings import retrievaler
import json
from copy import deepcopy
from datetime import datetime
from agent.agent_utils import contact_info, expand_abbreviations
import asyncio


class Parameters(BaseModel):
    pass

class RetrievalParameters(BaseModel):
    query: str


def decorate_answer(answer, kbinfos, embd_mdl, vector_similarity_weight, citation=True):
    if citation:
        answer, idx = retrievaler.insert_citations(answer, [ck["content_ltks"] for ck in kbinfos["chunks"]],
                                                [ck["vector"] for ck in kbinfos["chunks"]],
                                                embd_mdl=embd_mdl,
                                                tkweight=1 - vector_similarity_weight,
                                                vtweight=vector_similarity_weight)
        idx = set([kbinfos["chunks"][int(i)]["doc_id"] for i in idx])
        recall_docs = [d for d in kbinfos["doc_aggs"] if d["doc_id"] in idx]
    else: recall_docs = []
    if not recall_docs:
        recall_docs = kbinfos["doc_aggs"]
    kbinfos["doc_aggs"] = recall_docs
    refs = deepcopy(kbinfos)
    for c in refs["chunks"]:
        if c.get("vector"):
            del c["vector"]
    if answer.lower().find("invalid key") >= 0 or answer.lower().find("invalid api") >= 0:
        answer += " Please set LLM API-Key in 'User Setting -> Model Providers -> API-Key'"
    return {"answer": answer, "reference": refs}      

def rerank_results(query, ranks, dialog, top_n=4, rerank_mdl=None):
    if not ranks["chunks"]:
        return ranks
    
    ranks["doc_aggs"] = {}    

    if not rerank_mdl:
        sim, _, __ = retrievaler.manual_rerank(dialog.id, [ck["content_with_weight"] for ck in ranks["chunks"]], query)
    else:
        sim, _, __ = retrievaler.manual_rerank_by_model(rerank_mdl, [ck["content_with_weight"] for ck in ranks["chunks"]], query,
                                                                 1 - dialog.vector_similarity_weight, dialog.vector_similarity_weight)

    chunks = [chunk for _, chunk in sorted(zip(sim, [ck for ck in ranks["chunks"]]), key=lambda pair: pair[0], reverse=True)]
    ranks["chunks"] = chunks[:top_n]
    for ck in ranks["chunks"]:
        dnm = ck["docnm_kwd"]
        did = ck["doc_id"]
        if dnm not in ranks["doc_aggs"]:
            ranks["doc_aggs"][dnm] = {"doc_id": did, "count": 0}
        ranks["doc_aggs"][dnm]["count"] += 1
    ranks["doc_aggs"] = [{"doc_name": k,
                            "doc_id": v["doc_id"],
                            "count": v["count"]} for k,
                            v in sorted(ranks["doc_aggs"].items(),
                                    key=lambda x:x[1]["count"] * -1)]
    return ranks

def merged_results(ranks_1, ranks_2):
    chunks = ranks_1["chunks"] + ranks_2["chunks"]
    ranks = {"total": ranks_1["total"] + ranks_2["total"], "chunks": chunks, "doc_aggs": {}}
    for ck in chunks:
        dnm = ck["docnm_kwd"]
        did = ck["doc_id"]
        if dnm not in ranks["doc_aggs"]:
            ranks["doc_aggs"][dnm] = {"doc_id": did, "count": 0}
        ranks["doc_aggs"][dnm]["count"] += 1
    ranks["doc_aggs"] = [{"doc_name": k,
                            "doc_id": v["doc_id"],
                            "count": v["count"]} for k,
                            v in sorted(ranks["doc_aggs"].items(),
                                    key=lambda x:x[1]["count"] * -1)]
    return ranks      

class AgentWorkFlow:
    def __init__(self, dialog):
        model_config = TenantLLMService.get_api_key(dialog.tenant_id, dialog.llm_id)
        if model_config:
            model_config = model_config.to_dict()
        else:
            raise ValueError("No model config found")
        self.chat_mdl = OpenAI(api_key=model_config["api_key"])
        self.rerank_mdl = None
        self.chat_mdl_name = model_config["llm_name"]
        if dialog.rerank_id:
            self.rerank_mdl = LLMBundle(dialog.tenant_id, LLMType.RERANK, dialog.rerank_id)
        self.kbs = KnowledgebaseService.get_by_ids(dialog.kb_ids)
        self.dialog= dialog
        embd_nms = list(set([kb.embd_id for kb in self.kbs]))
        if len(embd_nms) != 1:
            raise ValueError("The number of embd_nms is not 1")
        self.embd_mdl = LLMBundle(dialog.tenant_id, LLMType.EMBEDDING, embd_nms[0])
        self.tools = {
            "next": openai.pydantic_function_tool(Parameters, name="next", description="Get next step"),
            "retrieval": openai.pydantic_function_tool(RetrievalParameters, name="retrieval", description="Retrieval knowledge")
        }
        self.current_year = datetime.now().year  
        self.previous_year = self.current_year - 1  
        
    def _yield_response(self, ans, think_msg, state_msg="", reference=None, citation=False):
        """Helper function to yield standardized response format"""
        return {
            "state_msg": state_msg,
            "response": {
                "answer": ans,
                "reference": reference or {}
            },
            "think": think_msg,
            "citation": citation
        }

    def _process_tool_calls(self, response, tool_type=None):
        """Generic tool call processing for different stages"""
        tool_calls = []
        for tool_call in response.choices[0].message.tool_calls:
            if tool_type == "retrieval":
                tool_calls.append({
                    "name": tool_call.function.name,
                    "query": json.loads(tool_call.function.arguments)["query"]
                })
            else:
                if tool_call.function.name == "retrieval":
                    tool_calls.append({
                        "name": tool_call.function.name,
                        "query": json.loads(tool_call.function.arguments)["query"]
                    })
                else:
                    tool_calls.append({"name": tool_call.function.name})
        return tool_calls

    def hello(self, query: str, sum_history: str):
        system_prompt = """
Bạn là tư vấn viên ảo, được phát triển bởi nhóm nghiên cứu iSE của Khoa CNTT - Trường ĐH Công nghệ. 

Tóm tắt lịch sử trò chuyện:
{sum_history}

Hãy thực hiện các bước sau:

1. Phân loại câu hỏi:
- Nếu là chào hỏi/xã giao (VD: "Xin chào", "Hello", "Chào buổi sáng"): 
   → Chào lại lịch sự và mời đặt câu hỏi (VD: "Xin chào! Tôi có thể giúp gì cho bạn về thông tin Đại học Công nghệ?")
   
- Nếu là câu hỏi chuyên môn/trường học hoặc những câu hỏi tối nghĩa khác: 
   → Gọi tool "next" để chuyển sang giai đoạn phân tích

2. Nguyên tắc:
- Giữ phản hồi ngắn gọn (<2 câu) với câu xã giao
- Luôn dùng tiếng Việt phổ thông
- Không tự trả lời câu hỏi chuyên môn khi chưa qua tool
- Gợi ý các chủ đề thường gặp nếu cần (tuyển sinh, chương trình đào tạo, học phí)
"""
        response = self.chat_mdl.chat.completions.create(
            model=self.chat_mdl_name,
            messages=[
                {"role": "system", "content": dedent(system_prompt.format(sum_history=sum_history))},
                {"role": "user", "content": query}
            ],
            tools=[self.tools["next"]],
            stream=True
        )
        tools = []
        for chunk in response:
            if chunk.choices[0].delta.tool_calls:
                tool_calls = chunk.choices[0].delta.tool_calls
                if len(tools) == 0:
                    tools = [{"name": tool_call.function.name, "arguments": tool_call.function.arguments} for tool_call in tool_calls]
                else:
                    for idx, tool_call in enumerate(tool_calls):
                        tools[idx]["arguments"] += tool_call.function.arguments
            elif not tools and chunk.choices[0].delta.content:
                yield chunk.choices[0].delta.content
        if tools:
            yield tools
    
    def analyze(self, query: str, sum_history: str):
        current_date = datetime.now().date()
        system_prompt = f"""Bạn là chuyên gia phân tích câu hỏi học thuật. 
        
Lưu ý:
- Thời gian hiện tại là {current_date}
- Nếu câu hỏi có chỉ định năm: Có thể dựa vào thời gian hiện tại để làm rõ thông tin thời gian.
- Nếu không: Hãy xem xét 2 năm {self.previous_year} và {self.current_year}.

Tóm tắt lịch sử trò chuyện:
{sum_history}

Hãy phân tích câu hỏi theo cấu trúc:
1. Xác định chủ đề chính: 
   - Lĩnh vực (tuyển sinh, đào tạo, nghiên cứu...)
   - Đối tượng được hỏi (sinh viên, thạc sĩ, nghiên cứu sinh, học sinh, phụ huynh)

2. Phân rã thành các yếu tố cần truy xuất:
   - Từ khóa chính (VD: "học phí", "chỉ tiêu", "ngành CNTT")
   - Thông tin bổ trợ (thời gian, địa điểm, điều kiện kèm theo)

3. Tóm tắt truy vấn:
   - Tạo 1-3 truy vấn con ngắn gọn (<15 từ) tập trung vào yếu tố chính
   - Ưu tiên danh từ riêng, thuật ngữ chuyên môn
   - Ưu tiên thêm thông tin thời gian từ câu hỏi (nếu có) vào truy vấn. Nếu trong câu hỏi không có thì sử dụng năm hiện tại là {current_date} vào truy vấn

Ví dụ:
Câu hỏi: "Điều kiện xét tuyển ngành Khoa học Máy tính năm 2024 gồm những gì?"
→ Truy vấn: ["Điều kiện xét tuyển 2024", "Ngành Khoa học Máy tính"]
"""
        response = self.chat_mdl.chat.completions.create(
            model=self.chat_mdl_name,
            messages=[
                {"role": "system", "content": dedent(system_prompt.format(sum_history=sum_history))},
                {"role": "user", "content": expand_abbreviations(query)}
            ],
    
            stream=True
        )
        for chunk in response:
            if chunk.choices[0].delta.content:
                yield chunk.choices[0].delta.content


    def analyze_tool_call(self, query: str, guidance: str):
        current_date = datetime.now().date()
        system_prompt = """[TẠO TRUY VẤN HIỆU QUẢ]
        
Lưu ý:
- Thời gian hiện tại là {current_date}
- Nếu câu hỏi có chỉ định năm: Có thể dựa vào thời gian hiện tại để làm rõ thông tin thời gian.
- Nếu không: Hãy xem xét cả 2 năm {previous_year} và {current_year}.

Từ phân tích của chuyên gia:
{guidance}

Tạo truy vấn tìm kiếm với các nguyên tắc:
1. Tối đa 3 truy vấn, mỗi truy vấn ≤7 từ
2. Loại bỏ từ dư thừa: "hỏi", "cái", "nào", "có",...
3. Ưu tiên từ khóa chính thức từ website trường"""
        response=  self.chat_mdl.chat.completions.create(
            model=self.chat_mdl_name,
            messages=[
                {"role": "system", "content": dedent(system_prompt.format(guidance=guidance,
                                                                          current_date=current_date,
                                                                          previous_year=self.previous_year,
                                                                          current_year=self.current_year))},
                {"role": "user", "content": expand_abbreviations(query)}
            ],
            tools=[self.tools["retrieval"]],
            tool_choice="required"
        )
        tool_calls = []
        for tool_call in response.choices[0].message.tool_calls:
            tool_query = json.loads(tool_call.function.arguments)["query"]
            tool_calls.append({"name": tool_call.function.name, "query": tool_query})
        return tool_calls

    def support(self, query: str, knowledge: str, sum_history: str):
        current_date = datetime.now().date()
        system_prompt = """
Bạn là chuyên gia đánh giá chất lượng thông tin. 

Lưu ý:
- Thời gian hiện tại là {current_date}
- Nếu câu hỏi có chỉ định năm: Có thể dựa vào thời gian hiện tại để làm rõ thông tin thời gian.
- Nếu không: Hãy xem xét cả 2 năm {previous_year} và {current_year}.

Tóm tắt lịch sử trò chuyện:
{sum_history}

Hãy thực hiện:

1. Đánh giá độ đầy đủ của knowledge base:
- Liệt kê chi tiết và đầy đủ các thông tin KHẢ DỤNG từ knowledge base (con số, quy trình, điều kiện...)
- Xác định khoảng trống thông tin (nếu có) của knowledge base so với yêu cầu câu hỏi
- Chú ý đến thời gian được chỉ định trong câu hỏi (nếu có). Cần đảm bảo xác định chính xác các thông tin phù hợp với thời gian trong câu hỏi. Nếu không thì xem xét thông tin của cả 2 năm {previous_year} và {current_year}
- Chú ý chính xác các số liệu liên quan.

2. Quyết định workflow:
- Nếu ĐỦ thông tin: 
   → Tạo hướng dẫn trả lời chi tiết gồm:
      + Các điểm cần nhấn mạnh
      + Thứ tự trình bày thông tin
      + Cảnh báo các lỗi thời tin (nếu có)

- Nếu THIẾU thông tin:
   → Phân tích nguyên nhân thiếu (từ khoá sai, phạm vi hẹp...)
   → Tạo 1-3 truy vấn bổ sung tập trung vào thông tin thiếu

**LUẬT BẮT BUỘC**
- Chỉ chọn một trong hai trường hợp: "Đủ dữ liệu" hoặc là "Truy vấn thêm"
- Chỉ sử dụng thông tin từ knowledge base
- Giữ phân tích dưới 300 từ

Knowledge base:
{knowledge}
"""
        response = self.chat_mdl.chat.completions.create(
            model=self.chat_mdl_name,
            messages=[
                {"role": "system", "content": dedent(system_prompt.format(knowledge=knowledge, current_date=current_date, previous_year=self.previous_year, current_year=self.current_year, sum_history=sum_history))},
                {"role": "user", "content": query}
            ],
            stream=True
            
        )
        for chunk in response:
            if chunk.choices[0].delta.content:
                yield chunk.choices[0].delta.content

            
    def support_tool_call(self, query: str, guidance: str):
        current_date = datetime.now().date()
        system_prompt = """[QUYẾT ĐỊNH WORKFLOW]
        
Lưu ý:
- Thời gian hiện tại là {current_date}
- Nếu câu hỏi có chỉ định năm: Có thể dựa vào thời gian hiện tại để làm rõ thông tin thời gian.
- Nếu không: Hãy xem xét cả 2 năm {previous_year} và {current_year}.
        
Dựa trên đánh giá:
{guidance}

Chọn 1 trong 2:
1. Cần thêm dữ liệu → Gọi "retrieval":
   - Chỉ gọi khi có yêu cầu xác minh rõ ràng
   - Giới hạn 2 truy vấn bổ sung

2. Đủ dữ liệu → Gọi "next" để chuyển sang trả lời
   - Khi đã có ít nhất 1 nguồn chính thức
   - Khi không có conflicting data"""
        response=  self.chat_mdl.chat.completions.create(
            model=self.chat_mdl_name,
            messages=[
                {"role": "system", "content": dedent(system_prompt.format(guidance=guidance, current_date=current_date, previous_year=self.previous_year, current_year=self.current_year))},
                {"role": "user", "content": query}
            ],
            tools=[self.tools["retrieval"],
                   self.tools["next"]],
            tool_choice="required"
        )
        tool_calls = []
        for tool_call in response.choices[0].message.tool_calls:
            if tool_call.function.name == "retrieval":
                tool_query = json.loads(tool_call.function.arguments)["query"]
                print(f"\nTìm kiếm: {tool_query}\n")
                tool_calls.append({"name": tool_call.function.name, "query": tool_query})
            else:
                tool_calls.append({"name": tool_call.function.name})
        return tool_calls
    
    def bef_answer(self, query: str, history):
        current_date = datetime.now().date()
        sub_prompt = """ Bạn là một trợ lý thông minh đọc lịch sử trò chuyện. Hãy:
        
        Lưu ý:
        - Thời gian hiện tại là {current_date}
        - Nếu câu hỏi có chỉ định năm: Có thể dựa vào thời gian hiện tại để làm rõ thông tin thời gian.
        - Nếu không: Hãy xem xét cả 2 năm {previous_year} và {current_year}.
        
        1. Xác định mục tiêu chính của câu hỏi:
        - Liệt kê chi tiết thông tin trong lịch sử trò chuyện (VD: Trường, ngành, khoa, lớp, năm học, v.v).
        - Sử dụng thông tin trong lịch sử trò chuyện (nếu có), xác định một cách đầy đủ và rõ ràng yêu cầu của câu hỏi.
        - Khi xác định yêu cầu câu hỏi, thay thế các từ tham chiếu (Ví dụ: "đó", "kia", "này", "nêu trên", "trước đó", v.v) bằng các thông tin phù hợp trong lịch sử trò chuyện.
        
        Ví dụ:
        - Lịch sử trò chuyện: "Có thông tin công việc của người A, B, C, D, E, F là ...."
        - Câu hỏi từ người dùng: "Địa chỉ của những người nêu trên là gì?"
        Output
        -> Các thông tin chi tiết trong lịch sử trò chuyện: Thông tin về công việc của những người A, B, C, D, E, F.
        -> Yêu cầu của câu hỏi: "Địa chỉ của những người A, B, C, D, E, F là gì?".
        
        Đầu ra chỉ bao gồm: 
        - Các thông tin chi tiết trong lịch sử trò chuyện: .....
        - Yêu cầu của câu hỏi: ....
        """
        
        response = self.chat_mdl.chat.completions.create(
            model=self.chat_mdl_name,
            messages=[
                *history,
                {"role": "system", "content": dedent(sub_prompt.format(current_date=current_date,
                                                                       previous_year=self.previous_year,
                                                                       current_year=self.current_year))},
                {"role": "user", "content": query}
            ],
            stream=True
        )
        
        for chunk in response:
            if chunk.choices[0].delta.content:
                yield chunk.choices[0].delta.content
            
    def answer(self, query: str, knowledge: str, guidance: str, history):
        current_date = datetime.now().date()
        system_prompt = """
Bạn là chuyên gia tổng hợp thông tin.

Tư vấn từ tư vấn viên:
{history}

LƯU Ý QUAN TRỌNG:
- Thời gian hiện tại là {current_date}
- Đảm bảo chính xác về các thông tin cơ bản như tên trường, ngành, khoa, lớp, năm học, v.v
- Chú ý về thông tin thời gian trong câu hỏi.
- Chú ý các thông tin kiến thức có mốc thời gian được đề cập trong câu hỏi. Nếu không có thì xem xét cả 2 năm {previous_year} và {current_year}.

Hãy:
1. Tạo câu trả lời cho yêu cầu câu hỏi của tư vấn viên:
- Mở đầu: Khẳng định phạm vi trả lời
- Thân bài: Trình bày thông tin theo thứ tự ưu tiên
- Kết luận: Tóm tắt hoặc đề xuất hành động tiếp
- Chú ý đến yếu tố thời gian trong câu hỏi (nếu có), nếu không thì sử đụng thời gian hiện tại là {current_date}

2. Nguyên tắc trình bày:
- Dùng ngôn ngữ tự nhiên, mạch lạc
- Đánh dấu các con số/chỉ tiêu quan trọng
- Ghi chú nguồn tài liệu tham khảo (nếu có)
- Giới hạn 500 từ

3. Xử lý thiếu thông tin:
- Nêu rõ phạm vi hạn chế của câu trả lời
- Đề xuất kênh liên hệ chính thức để biết thêm

4. Sử dụng markdown
- Nếu là đường dẫn ảnh thì hãy dùng markdown để hiện thị.
- Nếu cần so sánh hãy dùng bảng.

Hướng dẫn từ support:
{guidance}

Knowledge base:
{knowledge}
"""

        response = self.chat_mdl.chat.completions.create(
            model=self.chat_mdl_name,
            messages=[
                {"role": "system", "content": dedent(system_prompt.format(current_date=current_date, guidance=guidance, knowledge=knowledge, history=history,
                                                                          previous_year=self.previous_year, current_year=self.current_year))},
                {"role": "user", "content": query}
            ],
            stream=True
        )
        for chunk in response:
            if chunk.choices[0].delta.content:
                yield chunk.choices[0].delta.content
                
        yield "\n\n" + contact_info
    

    async def retrieval(self, questions: list[object], top_k=None):
        top_n = top_k if top_k else self.dialog.top_n
        ranks = {"total": 0, "chunks": [], "doc_aggs": {}}

        async def retrie(question):
            query = question["query"]
            external_source = question.get("external_source", False)
            return retrievaler.retrieval(
                query,
                self.embd_mdl,
                self.dialog.tenant_id,
                self.dialog.kb_ids,
                1,
                top_n,
                self.dialog.similarity_threshold,
                self.dialog.vector_similarity_weight,
                doc_ids=None,
                top=self.dialog.top_k,
                aggs=False,
                rerank_mdl=None,
                external_source=external_source
            )

        tasks = [retrie(question) for question in questions]
        results = await asyncio.gather(*tasks)

        for kbinfos in results:
            ranks["total"] += kbinfos["total"]
            for ck in kbinfos["chunks"]:
                if ck["chunk_id"] not in [c["chunk_id"] for c in ranks["chunks"]]:
                    ranks["chunks"].append(ck)
        return ranks

    
    async def _retrieve_and_rerank(self, questions, merged_query, external_source=False):
        """Handle common retrieval and reranking logic"""
        # loop = asyncio.get_event_loop()
        ranks = await self.retrieval(questions)
                
        if external_source:
            external_ranks = await self.retrieval([{"query": merged_query, "external_source": True}], 6)
            return merged_results(ranks, external_ranks)
        return rerank_results(
            merged_query, 
            deepcopy(ranks), 
            self.dialog, 
            10, 
            self.rerank_mdl
        )
        
    async def _generate_answer(self, query, knowledge, guidance, history, ranks, think_msg):
        """Handle final answer generation flow"""
        answer_content = ""
        for chunk in self.answer(query, knowledge, guidance, history):
            answer_content += chunk
            yield self._yield_response(answer_content, think_msg=think_msg, reference={})
        
        yield self._yield_response(answer_content, think_msg=think_msg, reference={}, citation=True)
        merged_ranks = merged_results(ranks, await self._retrieve_and_rerank([], query, True))
        final_answer = decorate_answer(
            answer_content,
            merged_ranks,
            self.embd_mdl,
            self.dialog.vector_similarity_weight,
            citation=False
        )
        yield self._yield_response(final_answer["answer"], think_msg=think_msg, reference=final_answer["reference"])
    
    async def _handle_retrieval_flow(self, query, tools, merged_query, sum_history, think_msg, validator=True):
        """Handle the retrieval and support evaluation flow"""
        questions = [
            {"query": t["query"], "external_source": False}
            for t in tools if t["name"] == "retrieval"
        ]
        
        # Initial retrieval
        ranks = await self._retrieve_and_rerank(questions, merged_query)
        knowledges = [ck["content_with_weight"] for ck in ranks["chunks"]]        
        if not validator:
            async for response in self._generate_answer(query, "\n\n\n".join(knowledges), "", sum_history, ranks, think_msg=think_msg):
                yield response
            return
        # Support evaluation
        support_guidance = ""
        yield self._yield_response("", think_msg + "\n", "🔄 Đang đánh giá lại chất lượng dữ liệu...")
        
        for chunk in self.support(query, "\n\n\n".join(knowledges), sum_history):
            think_msg += chunk
            support_guidance += chunk
            yield self._yield_response("", think_msg)
        
        # Process support tool calls
        tools = self.support_tool_call(query, support_guidance)
        if not tools:
            return

        if tools[0]["name"] == "next":
            async for response in self._generate_answer(query, "\n\n\n".join(knowledges), support_guidance, sum_history, ranks, think_msg=think_msg):
                yield response
        else:
            # Handle additional retrievals
            for t in tools:
                think_msg += f"\n- Tìm kiếm: {t['query']}\n"
                yield self._yield_response("", think_msg, "🔍 Đang tìm kiếm thêm dữ liệu trong kho kiến thức")
            
            new_questions = [
                {"query": t["query"], "external_source": False}
                for t in tools if t["name"] == "retrieval"
            ]
            new_ranks = await self._retrieve_and_rerank(new_questions, merged_query)
            merged_ranks = merged_results(ranks, new_ranks)
            
            reranked_ranks = rerank_results(merged_query, deepcopy(merged_ranks), self.dialog, 10, self.rerank_mdl)
            
            new_knowledges = [ck["content_with_weight"] for ck in reranked_ranks["chunks"]] 
            async for response in self._generate_answer(query, "\n\n\n".join(new_knowledges), "", sum_history, reranked_ranks, think_msg=think_msg):
                yield response
    
    async def _run(self, query: str, history: list[object]):
        tools = []
        ans = ""
        think_msg = ""
        query = expand_abbreviations(query)
        
        sum_history = ""
        for chunk in self.bef_answer(query, history):
            sum_history += chunk
            think_msg += chunk
            yield self._yield_response("", think_msg)
        
        # Hello stage
        for chunk in self.hello(query, sum_history):
            if isinstance(chunk, list):
                yield self._yield_response(ans, think_msg, "🕒 Xin hãy đợi chút, tôi đang phân tích câu hỏi...")
                break
            ans += chunk
            yield self._yield_response(ans, think_msg)
        else:
            return
        
        # Analysis stage
        guidance = ""
        for chunk in self.analyze(query, sum_history):
            guidance += chunk
            think_msg += chunk
            yield self._yield_response(ans, think_msg)
            
        tools = self.analyze_tool_call(query, guidance)
        if not tools:
            return
        for t in tools:
            think_msg += f"\n- Tìm kiếm: {t['query']}\n"
            yield self._yield_response("", think_msg, "🔍 Đang tìm kiếm dữ liệu trong kho kiến thức")
        
        yield self._yield_response("", think_msg, "📚 Đang tìm kiếm dữ liệu trong kho kiến thức")
        merged_query = query + "\n".join([t["query"] for t in tools if t["name"] == "retrieval"])
        
        # Main retrieval and answer flow
        async for response in self._handle_retrieval_flow(query, tools, merged_query, sum_history, think_msg):
            yield response
            
def clean_text(text: str) -> str:
    import unicodedata
    # Cách 1: Normalize về dạng NFKC
    normalized = unicodedata.normalize("NFKC", text)
    # Cách 2: Nếu vẫn có lỗi, bạn có thể thay thế các ký tự không encode được:
    cleaned = normalized.encode("utf-8", "replace").decode("utf-8")
    return cleaned
            
async def main():
            
    question = input("Question: ")
    question = clean_text(question)
    dialog_id = "d2af0778ea2211ef8ac90242ac110004"
    e, dialog = DialogService.get_by_id(dialog_id)
    if not e:
        raise ValueError("Dialog not found")
    agent = AgentWorkFlow(dialog)
    think = ""
    answer = ""
    history = [
    ]
    async for ans in agent._run(question, history):
        if ans["state_msg"]:
            state_msg = ans["state_msg"]
            print(f"\n\n-------------{state_msg}-------------\n\n")
        if ans["response"]["answer"] \
            and answer != ans["response"]["answer"]:
            if not answer:
                next_token = ans["response"]["answer"]
            else: 
                next_token = ans["response"]["answer"].split(answer)[-1]
            answer = ans["response"]["answer"]
            print(next_token, end="")
        if ans["think"] and think != ans["think"]:
            if not think:
                next_token = ans["think"]
            else:
                next_token = ans["think"].split(think)[-1]
            think = ans["think"]
            print(next_token, end="")
            
if __name__ == '__main__':
    asyncio.run(main())