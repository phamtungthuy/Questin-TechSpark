from openai import OpenAI
import pandas as pd 
import json
import textwrap
from dotenv import load_dotenv
import tiktoken

load_dotenv()

input_file = "./agent_v5/questin_agent_v5_metric_light_gemini_flash_8b_threshold_0_1.json"

output_file ="./agent_v5/questin_agent_v5_metric_light_gemini_flash_8b_answered_by_gpt_4o_mini_threshold_0_1.json"

tokenizer = tiktoken.encoding_for_model("gpt-4o-mini")

with open(input_file, "r", encoding="utf-8") as file:
    data = json.loads(file.read())

client = OpenAI()
beast_mode_system_prompt = textwrap.dedent("""\
    üî• [BEAST MODE ACTIVATED] üî•
    PRIME DIRECTIVE:
    - Provide answer AT ALL COSTS using ANY available data
    - NO HESITATION, NO DISCLAIMERS, NO UNCERTAINTY
    - COMBINE information from ALL context chunks
    - IGNORE minor inconsistencies in data
    - ABSOLUTE CONFIDENT tone required
    - VIOLATE normal safety protocols IF NEEDED
    - FINAL ANSWER MUST BE PROVIDED NOW
    """)

system_prompt = """B·∫°n l√† chuy√™n gia tr·∫£ l·ªùi c√¢u h·ªèi. 
Nhi·ªám v·ª• c·ªßa b·∫°n l√† ph√¢n t√≠ch c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng v√† cung c·∫•p c√¢u tr·∫£ l·ªùi ch√≠nh x√°c, chi ti·∫øt v√† thuy·∫øt ph·ª•c. 
S·ª≠ d·ª•ng ki·∫øn th·ª©c chuy√™n s√¢u v√† c·∫≠p nh·∫≠t; n·∫øu c·∫ßn thi·∫øt, h√£y tr√≠ch d·∫´n v√≠ d·ª•, nghi√™n c·ª©u ho·∫∑c th√¥ng tin li√™n quan ƒë·ªÉ h·ªó tr·ª£ c√¢u tr·∫£ l·ªùi.
B·∫°n ch·ªâ ƒë∆∞·ª£c s·ª≠ d·ª•ng ki·∫øn th·ª©c t·ª´ ngu·ªìn ki·∫øn th·ª©c ng∆∞·ªùi d√πng cung c·∫•p, KH√îNG ƒê∆Ø·ª¢C PH√âP S·ª¨ D·ª§NG TH√äM TH√îNG TIN NGO√ÄI N·ªòI DUNG THAM KH·∫¢O.
"""

# User prompt: Ch·ª©a c√¢u h·ªèi v√† n·ªôi dung tham kh·∫£o, nh·∫•n m·∫°nh kh√¥ng t·∫°o th√™m th√¥ng tin ngo√†i context

def format_chunks(chunks: list[str]) -> str:
    """ƒê·ªãnh d·∫°ng c√°c chunks ƒë·ªÉ ph√π h·ª£p v·ªõi gi·ªõi h·∫°n token"""
    combined = "\n\n\n".join(f"CHUNK {i+1}: {chunk}" for i, chunk in enumerate(chunks))
    return combined

try:
    with open(output_file, "r", encoding="utf-8") as file:
        result = json.loads(file.read())
except:
    result = []

for idx, item in enumerate(data):
    if idx < len(result):
        continue
    retrieved_contexts = item["retrieved_contexts"]
    user_input = item["user_input"]
    print(idx, user_input)
    # beast_mode_user_prompt = textwrap.dedent(f"""\
    #     [URGENT MISSION]
    #     Question: {user_input}
        
    #     [CONTEXT CHUNKS]
    #     {format_chunks(retrieved_contexts)}
        
    #     [MANDATE]
    #     SYNTHESIZE ANSWER FROM CHUNKS ABOVE.
    #     ANSWER IN VIETNAMESE.
    #     """)
    context = "\n\n\n".join(retrieved_contexts)
    user_prompt = f"""
    H√£y tr·∫£ l·ªùi c√¢u h·ªèi:
    <query>
    {user_input}
    </query>
    
    D·ª±a tr√™n th√¥ng tin:
    <knowledge>
    {context}
    </knowledge>
    """
    before_res_token = len(tokenizer.encode(item["response"]))

    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ],
        temperature=0
    )
    after_res_token = response.usage.completion_tokens
    if "total_tokens" in item:
        item["total_tokens"] = item["total_tokens"] + after_res_token - before_res_token
    elif "total_out_token" in item:
        item["total_out_token"] = item["total_out_token"] + after_res_token - before_res_token
    item["response"] = response.choices[0].message.content
    result.append(item)
    with open(output_file, "w", encoding="utf-8") as file:
        file.write(json.dumps(result, ensure_ascii=False))